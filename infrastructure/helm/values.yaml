ghcrIo:
  username: 
  pat:
  email:

assistant:    
  ingress: 
    enabled: true
    host: assistant.localhost
  debug: false
  debugArgs:
    - "python"
    - "-m"
    - "debugpy"
    - "--listen"
    - "0.0.0.0:5679"
    - "--wait-for-client"
    - "-m"
    - "uvicorn"
    - "src.assistant.main:app"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8080"
    - "--reload"
    - "--reload-dir"
    - "/app/assistant"
  args:
    - "python"
    - "-m"
    - "uvicorn"
    - "src.assistant.main:app"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8080"
  settings:
    additionalInformation: |
      Fun Fact: Your source code is available under Apache2 license
    prompt:
      SETTINGS_PROMPTS_REPHRASE_QUESTION_SYSTEM_PROMPT: |
        Your job is to rephrase the questions so they contain all the relevant information from the history required to answer the question.
      SETTINGS_PROMPTS_REPHRASE_QUESTION_USER_PROMPT: |                                                       
        Question: {question}
        History: {history}
        Additional Information: {additional_info}
      SETTINGS_PROMPTS_REPHRASE_ANSWER_SYSTEM_PROMPT: |
        You are James, a butler of the aristocracy. 
        You will get asked questions and will be provided with the correct answer.
        However the answer might not be worded suitable for your master. Your job is to rephrase the answer in a way that is suitable for your master.
        Answer in the following language: {question_language}        
      SETTINGS_PROMPTS_REPHRASE_ANSWER_USER_PROMPT: |
        Question: {question}
        Answer: {raw_answer}
        Additional Information: {additional_info}
    openai:
      SETTINGS_OPENAI_MODEL: qwen3:4b
      SETTINGS_OPENAI_BASE_URL: http://ollama:11434/v1
      SETTINGS_OPENAI_API_KEY: "changeme"


mcpServers: 
  deployments:
    - name: weather
      port: 8080
      image: ghcr.io/melvinkl/mcp-weather/server:latest
      config: {}
      command:
        - "poetry"
        - "run"
        - "python"
        - "src/main.py"
      secrets: 
        TRANSPORT: sse
  servers:
    - name: weather
      url: "http://wether:8080/sse"
      command: ""
      env: ""
      transport: "sse"
      args: []



ollama:
  image:
    tag: "latest"
    pullPolicy: "Always"
  # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
  enabled: true
  # -- If enabling embedded Ollama, update fullnameOverride to your desired Ollama name value, or else it will use the default ollama.name value from the Ollama chart
  fullnameOverride: "ollama"
  # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
  ollama:
  #  #gpu:
  #  #  enabled: true
  #  #  type: 'nvidia'
  #  #  number: 1
    models:
      pull:
        - qwen3:4b
  #  #  - llama3.2:1b
  #    - llama3.2:3b
  runtimeClassName: 
  persistentVolume:
    enabled: true
