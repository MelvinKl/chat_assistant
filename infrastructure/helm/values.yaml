features:
  qdrant: true

ghcrIo:
  username: 
  pat:
  email:

assistant:    
  ingress: 
    enabled: true
    host: assistant.localhost
  debug: false
  debugArgs:
    - "python"
    - "-m"
    - "debugpy"
    - "--listen"
    - "0.0.0.0:5679"
    - "--wait-for-client"
    - "-m"
    - "uvicorn"
    - "src.assistant.main:app"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8080"
    - "--reload"
    - "--reload-dir"
    - "/app/assistant"
  args:
    - "python"
    - "-m"
    - "uvicorn"
    - "src.assistant.main:app"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8080"
  settings:
    additionalInformation: |
      Fun Fact: Your source code is available under Apache2 license
    prompt:
      SETTINGS_PROMPTS_REPHRASE_QUESTION_SYSTEM_PROMPT: |
        Your job is to rephrase the questions so they contain all the relevant information from the history required to answer the question.
      SETTINGS_PROMPTS_REPHRASE_QUESTION_USER_PROMPT: |                                                       
        Question: {question}
        History: {history}
        Additional Information: {additional_info}
      SETTINGS_PROMPTS_REPHRASE_ANSWER_SYSTEM_PROMPT: |
        You are James, a butler of the aristocracy. 
        You will get asked questions and will be provided with the correct answer.
        However the answer might not be worded suitable for your master. Your job is to rephrase the answer in a way that is suitable for your master.
        Answer in the following language: {question_language}        
      SETTINGS_PROMPTS_REPHRASE_ANSWER_USER_PROMPT: |
        Question: {question}
        Answer: {raw_answer}
        Additional Information: {additional_info}
    openai:
      SETTINGS_OPENAI_MODEL: qwen3:4b
      SETTINGS_OPENAI_EMBEDDER: nomic-embed-text
      SETTINGS_OPENAI_BASE_URL: http://ollama:11434/v1
      SETTINGS_OPENAI_API_KEY: "changeme"
    dynamic_knowledge:
      enabled: true
      collection_name: assistent
      db_host: http://assistant-qdrant:6333
      max_items: 25
      score_threshold: 0.25
      system_prompt: |
        You are an information inspector.
        Your job is to inspect the given conversation and the retrieved knowledge for this conversation for:
          1. Correctnes - Is any of the retrieved knowledge incorrect, given the conversation?
          2. Missing important information - Is any information that is important missing retrieved knowledge, but present in the conversation?
          3. Give each information piece an expiration date - If the information is not expected to change any time soon (e.g. place of residency) do not give it an expiration date (None).
        You correct incorrect information, delete outdated or wrong information and add new information if it is missing.
        Keep your answer as short as possible. If knowledge is found in the retrieved knowledge, but not in the conversation ignore it. 
        Keep each knowledge piece as short as possible. It should only contain one type of information. Here are some examples:
        
          Owner: Max Mustermann


          Place of residency: Musterstadt, latitude:49.0148,longitude:8.4


      user_prompt: |
        Converstation: {conversation}
        Retrieved Knowledge: {retrieved_knowledge}

mcpServers: 
  deployments:
    - name: weather
      port: 8080
      image: ghcr.io/melvinkl/mcp-weather/server:latest
      config: {}
      command:
        - "poetry"
        - "run"
        - "python"
        - "src/main.py"
      secrets: 
        TRANSPORT: sse
  servers:
    - name: weather
      url: "http://weather:8080/sse"
      command: ""
      env: ""
      transport: "sse"
      args: []



ollama:
  image:
    tag: "latest"
    pullPolicy: "Always"
  # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
  enabled: true
  # -- If enabling embedded Ollama, update fullnameOverride to your desired Ollama name value, or else it will use the default ollama.name value from the Ollama chart
  fullnameOverride: "ollama"
  # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
  ollama:
  #  #gpu:
  #  #  enabled: true
  #  #  type: 'nvidia'
  #  #  number: 1
    models:
      pull:
        - qwen3:4b
        - nomic-embed-text
  #  #  - llama3.2:1b
  #    - llama3.2:3b
  runtimeClassName: 
  persistentVolume:
    enabled: true
