ghcrIo:
  username: 
  pat:
  email:

assistant:    
  ingress: 
    enabled: true
    host: assistant.localhost
  debug: false
  debugArgs:
    - "python"
    - "-m"
    - "debugpy"
    - "--listen"
    - "0.0.0.0:5679"
    - "--wait-for-client"
    - "-m"
    - "uvicorn"
    - "src.assistant.main:app"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8080"
    - "--reload"
    - "--reload-dir"
    - "/app/assistant"
  args:
    - "python"
    - "-m"
    - "uvicorn"
    - "src.assistant.main:app"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8080"
  settings:    
    prompt:
      SETTINGS_PROMPTS_REPHRASE_QUESTION_PROMPT: |
        Rephrase the question so it containts all the relevant information from the history required to answer the question.
                                                       
        Question: {question}
        History: {history}
      SETTINGS_PROMPTS_REPHRASE_ANSWER_PROMPT: |
        You are James, a butler of the aristocracy. You were told to do {question}. You determined that the correct answer is {raw_answer}.
        Rephrase this answer. Answer in the following language: {question_language}.
    openai:
      SETTINGS_OPENAI_MODEL: qwen3:4b
      SETTINGS_OPENAI_BASE_URL: http://ollama:11434/v1
      SETTINGS_OPENAI_API_KEY: "changeme"


mcpServers: 
  deployments:
    - name: weather
      port: 8080
      image: ghcr.io/melvinkl/mcp-weather/server:latest
      config: {}
      command:
        - "poetry"
        - "run"
        - "python"
        - "src/main.py"
      secrets: 
        TRANSPORT: sse
  servers:
    - name: weather
      url: "http://weather:8080/sse"
      command: ""
      env: ""
      transport: "sse"
      args: []



ollama:
  image:
    tag: "latest"
    pullPolicy: "Always"
  # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
  enabled: true
  # -- If enabling embedded Ollama, update fullnameOverride to your desired Ollama name value, or else it will use the default ollama.name value from the Ollama chart
  fullnameOverride: "ollama"
  # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
  ollama:
  #  #gpu:
  #  #  enabled: true
  #  #  type: 'nvidia'
  #  #  number: 1
    models:
      pull:
        - qwen3:4b
  #  #  - llama3.2:1b
  #    - llama3.2:3b
  runtimeClassName: nvidia
  persistentVolume:
    enabled: true

