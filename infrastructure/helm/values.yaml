ghcrIo:
  username: github-username # replace with your github username
  pat: github-pat # replace with your github personal access token
  email: email-address@domain.de # replace with your email address

assistant:    
  ingress: 
    enabled: true
    host: assistant.melvin.beer
  debug: false
  debugArgs:
    - "python"
    - "-m"
    - "debugpy"
    - "--listen"
    - "0.0.0.0:5679"
    - "--wait-for-client"
    - "-m"
    - "uvicorn"
    - "src.assistant.main:app"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8080"
    - "--reload"
    - "--reload-dir"
    - "/app/assistant"
  args:
    - "python"
    - "-m"
    - "uvicorn"
    - "src.assistant.main:app"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8080"
  settings:    
    openai:
      SETTINGS_OPENAI_MODEL: Qwen2.5:3b
      SETTINGS_OPENAI_BASE_URL: http://ollama:11434/v1
      SETTINGS_OPENAI_API_KEY: "changeme"


mcpServers: 
  deployments:
    - name: weather
      port: 8080
      image: ghcr.io/melvinkl/mcp-weather/server:latest
      config: {}
      command:
        - "poetry"
        - "run"
        - "python"
        - "src/main.py"
      secrets: 
        TRANSPORT: sse
  servers:
    - name: weather
      url: "http://weather:8080/sse"
      command: ""
      env: ""
      transport: "sse"
      args: []



ollama:
  image:
    tag: "latest"
    pullPolicy: "Always"
  # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
  enabled: true
  # -- If enabling embedded Ollama, update fullnameOverride to your desired Ollama name value, or else it will use the default ollama.name value from the Ollama chart
  fullnameOverride: "ollama"
  # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
  ollama:
  #  #gpu:
  #  #  enabled: true
  #  #  type: 'nvidia'
  #  #  number: 1
    models:
      pull:
        - qwen3:4b
  #  #  - llama3.2:1b
  #    - llama3.2:3b
  runtimeClassName: nvidia
  persistentVolume:
    enabled: true

